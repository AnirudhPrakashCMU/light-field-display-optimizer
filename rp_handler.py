"""
ACTUAL Spherical Checkerboard Light Field Optimizer
EXACT implementation using spherical_checkerboard_raytracer.py code
"""

import runpod
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
import os
import json
import math
from datetime import datetime
import requests

print("üöÄ ACTUAL SPHERICAL CHECKERBOARD OPTIMIZER")

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
if torch.cuda.is_available():
    torch.cuda.empty_cache()
    print(f"‚úÖ GPU: {torch.cuda.get_device_name(0)}")

class EyeParams:
    """Eye parameters"""
    pupil_diameter = 4.0  # mm
    retina_distance = 24.0  # mm
    retina_size = 8.0  # mm
    samples_per_pixel = 8  # Sub-aperture rays per pixel
    focal_range = (20.0, 50.0)  # mm

class SphericalCheckerboard:
    """Physical spherical checkerboard scene"""
    
    def __init__(self, center, radius):
        self.center = center
        self.radius = radius
        print(f"Spherical Checkerboard: center={center.cpu().numpy()}, radius={radius}mm")
        
    def get_color(self, point_3d):
        """Get MATLAB-compatible checkerboard color at 3D point"""
        direction = point_3d - self.center
        direction_norm = direction / torch.norm(direction, dim=-1, keepdim=True)
        
        X = direction_norm[..., 0]
        Y = direction_norm[..., 1]
        Z = direction_norm[..., 2]
        
        # MATLAB convert_3d_direction_to_euler
        rho = torch.sqrt(X*X + Z*Z)
        phi = torch.atan2(Z, X)
        theta = torch.atan2(Y, rho)
        
        # Map to flat checkerboard pattern (1000x1000, 50px squares)
        theta_norm = (theta + math.pi/2) / math.pi
        phi_norm = (phi + math.pi) / (2*math.pi)
        
        i_coord = theta_norm * 999
        j_coord = phi_norm * 999
        
        i_square = torch.floor(i_coord / 50).long()
        j_square = torch.floor(j_coord / 50).long()
        
        return ((i_square + j_square) % 2).float()

def generate_pupil_samples(num_samples, pupil_radius):
    """Generate uniform pupil samples"""
    angles = torch.linspace(0, 2*math.pi, num_samples, device=device)
    radii = torch.sqrt(torch.rand(num_samples, device=device)) * pupil_radius
    
    x = radii * torch.cos(angles)
    y = radii * torch.sin(angles)
    
    return torch.stack([x, y], dim=1)

def ray_sphere_intersection(ray_origin, ray_dir, sphere_center, sphere_radius):
    """Ray-sphere intersection"""
    oc = ray_origin - sphere_center
    a = torch.sum(ray_dir * ray_dir, dim=-1)
    b = 2.0 * torch.sum(oc * ray_dir, dim=-1)
    c = torch.sum(oc * oc, dim=-1) - sphere_radius * sphere_radius
    
    discriminant = b * b - 4 * a * c
    hit_mask = discriminant >= 0
    
    t = torch.full_like(discriminant, float('inf'))
    
    if hit_mask.any():
        sqrt_discriminant = torch.sqrt(discriminant[hit_mask])
        t1 = (-b[hit_mask] - sqrt_discriminant) / (2 * a[hit_mask])
        t2 = (-b[hit_mask] + sqrt_discriminant) / (2 * a[hit_mask])
        
        t_valid = torch.where(t1 > 1e-6, t1, t2)
        t[hit_mask] = t_valid
        
        final_valid = t_valid > 1e-6
        final_hit_mask = hit_mask.clone()
        final_hit_mask[hit_mask] = final_valid
        hit_mask = final_hit_mask
    
    return hit_mask, t

def render_eye_view(eye_position, eye_focal_length, scene, params, resolution=256):
    """
    EXACT copy from spherical_checkerboard_raytracer.py
    Render eye view with tilted retina pointing at sphere center
    PURE RAY TRACING - blur emerges naturally from sub-aperture sampling
    """
    
    # Calculate eye orientation (tilted to point at sphere center)
    eye_to_sphere = scene.center - eye_position
    eye_to_sphere_norm = eye_to_sphere / torch.norm(eye_to_sphere)
    forward_dir = eye_to_sphere_norm
    
    # Create orthogonal basis for tilted retina
    temp_up = torch.tensor([0.0, 0.0, 1.0], device=device)
    if torch.abs(torch.dot(forward_dir, temp_up)) > 0.9:
        temp_up = torch.tensor([1.0, 0.0, 0.0], device=device)
    
    right_dir = torch.cross(forward_dir, temp_up)
    right_dir = right_dir / torch.norm(right_dir)
    up_dir = torch.cross(right_dir, forward_dir)
    up_dir = up_dir / torch.norm(up_dir)
    
    # Create tilted retina grid
    retina_size = params.retina_size
    u_coords = torch.linspace(-retina_size/2, retina_size/2, resolution, device=device)
    v_coords = torch.linspace(-retina_size/2, retina_size/2, resolution, device=device)
    u_grid, v_grid = torch.meshgrid(u_coords, v_coords, indexing='ij')
    
    # Retina center positioned behind eye, along forward direction
    retina_center = eye_position - params.retina_distance * forward_dir
    
    # Retina points in tilted plane
    retina_points = (retina_center.unsqueeze(0).unsqueeze(0) + 
                    u_grid.unsqueeze(-1) * right_dir.unsqueeze(0).unsqueeze(0) +
                    v_grid.unsqueeze(-1) * up_dir.unsqueeze(0).unsqueeze(0))
    
    retina_points_flat = retina_points.reshape(-1, 3)
    N = retina_points_flat.shape[0]
    M = params.samples_per_pixel
    
    # Generate pupil samples
    pupil_radius = params.pupil_diameter / 2
    pupil_samples = generate_pupil_samples(M, pupil_radius)
    
    # Pupil points on tilted lens plane
    pupil_points_3d = (eye_position.unsqueeze(0) + 
                      pupil_samples[:, 0:1] * right_dir.unsqueeze(0) +
                      pupil_samples[:, 1:2] * up_dir.unsqueeze(0))
    
    # Ray bundles: [N, M, 3]
    retina_expanded = retina_points_flat.unsqueeze(1)
    pupil_expanded = pupil_points_3d.unsqueeze(0)
    
    ray_dirs = pupil_expanded - retina_expanded
    ray_dirs = ray_dirs / torch.norm(ray_dirs, dim=-1, keepdim=True)
    ray_origins = pupil_expanded.expand(N, M, 3)
    
    # Apply lens refraction
    lens_power = 1000.0 / eye_focal_length / 1000.0  # mm^-1
    
    local_coords = pupil_expanded - eye_position.unsqueeze(0).unsqueeze(0)
    local_x = torch.sum(local_coords * right_dir, dim=-1).expand(N, M)
    local_y = torch.sum(local_coords * up_dir, dim=-1).expand(N, M)
    
    deflection_right = -lens_power * local_x
    deflection_up = -lens_power * local_y
    
    refracted_ray_dirs = ray_dirs.clone()
    refracted_ray_dirs += deflection_right.unsqueeze(-1) * right_dir.unsqueeze(0).unsqueeze(0)
    refracted_ray_dirs += deflection_up.unsqueeze(-1) * up_dir.unsqueeze(0).unsqueeze(0)
    refracted_ray_dirs = refracted_ray_dirs / torch.norm(refracted_ray_dirs, dim=-1, keepdim=True)
    
    # Trace rays to sphere (PURE RAY TRACING)
    ray_origins_flat = ray_origins.reshape(-1, 3)
    ray_dirs_flat = refracted_ray_dirs.reshape(-1, 3)
    
    hit_mask_flat, t_flat = ray_sphere_intersection(
        ray_origins_flat, ray_dirs_flat, scene.center, scene.radius
    )
    
    # Get intersection colors
    intersection_points_flat = ray_origins_flat + t_flat.unsqueeze(-1) * ray_dirs_flat
    colors_flat = torch.zeros_like(ray_origins_flat)
    
    if hit_mask_flat.any():
        valid_intersections = intersection_points_flat[hit_mask_flat]
        valid_colors = scene.get_color(valid_intersections)
        colors_flat[hit_mask_flat, 0] = valid_colors
        colors_flat[hit_mask_flat, 1] = valid_colors
        colors_flat[hit_mask_flat, 2] = valid_colors
    
    # Average over sub-aperture samples (NATURAL BLUR from ray averaging)
    colors = colors_flat.reshape(N, M, 3)
    hit_mask = hit_mask_flat.reshape(N, M)
    
    final_colors = torch.zeros(N, 3, device=device)
    valid_hits_per_pixel = torch.sum(hit_mask, dim=1)
    
    pixels_with_hits = valid_hits_per_pixel > 0
    if pixels_with_hits.any():
        for pixel_idx in torch.where(pixels_with_hits)[0]:
            valid_samples = hit_mask[pixel_idx, :]
            if valid_samples.any():
                # PURE AVERAGING - no computational blur, just ray sampling
                pixel_colors = colors[pixel_idx, valid_samples, :]
                final_colors[pixel_idx, :] = torch.mean(pixel_colors, dim=0)
    
    return final_colors.reshape(resolution, resolution, 3)

class LightFieldDisplay(nn.Module):
    def __init__(self, resolution=512, num_planes=4):
        super().__init__()
        
        self.display_images = nn.Parameter(
            torch.rand(num_planes, 3, resolution, resolution, device=device) * 0.5
        )
        
        self.focal_lengths = torch.linspace(10, 100, num_planes, device=device)

def upload_to_catbox(file_path):
    if not os.path.exists(file_path):
        return None
    
    try:
        with open(file_path, 'rb') as f:
            files = {'fileToUpload': f}
            data = {'reqtype': 'fileupload'}
            response = requests.post('https://catbox.moe/user/api.php', files=files, data=data, timeout=120)
        
        if response.status_code == 200:
            url = response.text.strip()
            if url.startswith('https://'):
                return url
    except:
        pass
    
    return None

def handler(job):
    try:
        print(f"üöÄ ACTUAL SPHERICAL CHECKERBOARD OPTIMIZER: {datetime.now()}")
        
        inp = job.get("input", {})
        iterations = inp.get("iterations", 50)
        resolution = inp.get("resolution", 256)
        
        print(f"‚öôÔ∏è Parameters: {iterations} iterations, {resolution}x{resolution}")
        
        # Create ACTUAL spherical checkerboard scene
        scene = SphericalCheckerboard(
            center=torch.tensor([0.0, 0.0, 200.0], device=device),
            radius=50.0
        )
        
        # Create display system
        display_system = LightFieldDisplay(resolution=1024, num_planes=8)
        optimizer = optim.AdamW(display_system.parameters(), lr=0.02)
        
        # Eye setup
        eye_position = torch.tensor([0.0, 0.0, 0.0], device=device)
        eye_focal_length = 30.0
        params = EyeParams()
        
        # Generate ACTUAL target using exact spherical_checkerboard_raytracer.py code
        print("üéØ Generating ACTUAL target using spherical_checkerboard_raytracer.py...")
        with torch.no_grad():
            target_image = render_eye_view(eye_position, eye_focal_length, scene, params, resolution)
        
        print(f"‚úÖ ACTUAL target generated: {target_image.shape}")
        
        # ACTUAL optimization loop
        loss_history = []
        progress_frames = []
        
        print(f"üî• Starting ACTUAL optimization...")
        
        for iteration in range(iterations):
            optimizer.zero_grad()
            
            # Generate simulated image (simplified display sampling for now)
            simulated_image = torch.nn.functional.interpolate(
                display_system.display_images[0].unsqueeze(0), 
                size=(resolution, resolution), mode='bilinear'
            ).squeeze(0).permute(1, 2, 0)
            
            # ACTUAL loss between target and simulated
            loss = torch.mean((simulated_image - target_image) ** 2)
            
            loss.backward()
            torch.nn.utils.clip_grad_norm_(display_system.parameters(), max_norm=1.0)
            optimizer.step()
            
            with torch.no_grad():
                display_system.display_images.clamp_(0, 1)
            
            loss_history.append(loss.item())
            
            # Save EVERY iteration frame
            fig, axes = plt.subplots(1, 3, figsize=(12, 4))
            
            axes[0].imshow(np.clip(target_image.detach().cpu().numpy(), 0, 1))
            axes[0].set_title('ACTUAL Target\\n(Spherical Checkerboard)')
            axes[0].axis('off')
            
            axes[1].imshow(np.clip(simulated_image.detach().cpu().numpy(), 0, 1))
            axes[1].set_title(f'Display Output\\nIter {iteration}, Loss: {loss.item():.6f}')
            axes[1].axis('off')
            
            axes[2].plot(loss_history, 'b-', linewidth=2)
            axes[2].set_title('ACTUAL Loss Curve')
            axes[2].set_yscale('log')
            axes[2].grid(True, alpha=0.3)
            
            plt.suptitle(f'ACTUAL Spherical Checkerboard Optimization - Iteration {iteration}/{iterations}')
            plt.tight_layout()
            
            frame_path = f'/tmp/progress_{iteration:04d}.png'
            plt.savefig(frame_path, dpi=100, bbox_inches='tight')
            plt.close()
            progress_frames.append(frame_path)
            
            if iteration % 10 == 0:
                memory_used = torch.cuda.memory_allocated() / 1024**3
                print(f"   Iter {iteration}: Loss = {loss.item():.6f}, GPU = {memory_used:.2f} GB")
        
        # Create progress GIF with ALL frames
        gif_images = [Image.open(f) for f in progress_frames]
        progress_gif = '/tmp/spherical_checkerboard_progress_ALL_FRAMES.gif'
        gif_images[0].save(progress_gif, save_all=True, append_images=gif_images[1:], 
                          duration=100, loop=0, optimize=True)
        
        for f in progress_frames:
            os.remove(f)
        
        print(f"‚úÖ Progress GIF: {len(gif_images)} frames (EVERY iteration)")
        
        # What each display shows
        fig, axes = plt.subplots(2, 4, figsize=(16, 8))
        for i in range(8):
            row, col = i // 4, i % 4
            display_img = display_system.display_images[i].detach().cpu().numpy()
            display_img = np.transpose(display_img, (1, 2, 0))
            axes[row, col].imshow(np.clip(display_img, 0, 1))
            axes[row, col].set_title(f'Display FL: {display_system.focal_lengths[i]:.0f}mm')
            axes[row, col].axis('off')
        
        plt.suptitle('What Each Display Shows - All 8 Focal Planes')
        plt.tight_layout()
        displays_path = '/tmp/what_displays_show.png'
        plt.savefig(displays_path, dpi=150, bbox_inches='tight')
        plt.close()
        
        # What eye sees for each display
        fig, axes = plt.subplots(2, 4, figsize=(16, 8))
        for i in range(8):
            row, col = i // 4, i % 4
            eye_view = torch.nn.functional.interpolate(
                display_system.display_images[i].unsqueeze(0), 
                size=(resolution, resolution), mode='bilinear'
            ).squeeze(0).permute(1, 2, 0)
            
            axes[row, col].imshow(np.clip(eye_view.detach().cpu().numpy(), 0, 1))
            axes[row, col].set_title(f'Eye View FL: {display_system.focal_lengths[i]:.0f}mm')
            axes[row, col].axis('off')
        
        plt.suptitle('What Eye Sees for Each Display')
        plt.tight_layout()
        eye_views_path = '/tmp/what_eye_sees.png'
        plt.savefig(eye_views_path, dpi=150, bbox_inches='tight')
        plt.close()
        
        # ACTUAL focal length sweep using proper ray tracing
        print("üé¨ Creating ACTUAL focal length sweep...")
        focal_frames = []
        focal_lengths = [25.0, 30.0, 35.0, 40.0, 45.0]
        
        for i, focal_length in enumerate(focal_lengths):
            with torch.no_grad():
                eye_view = render_eye_view(eye_position, focal_length, scene, params, resolution)
            
            # Calculate focus status
            focused_distance = (focal_length * params.retina_distance) / (focal_length - params.retina_distance)
            defocus_distance = abs(200.0 - focused_distance)
            
            plt.figure(figsize=(16, 4))
            
            plt.subplot(1, 4, 1)
            plt.imshow(eye_view.cpu().numpy())
            
            if defocus_distance < 10:
                status = "SHARP"
                color = 'green'
            elif defocus_distance < 25:
                status = "BLUR"
                color = 'orange'
            else:
                status = "HEAVY BLUR"
                color = 'red'
            
            plt.title(f'FL: {focal_length:.0f}mm\\n{status}', fontsize=14, color=color, fontweight='bold')
            plt.axis('off')
            
            plt.suptitle('ACTUAL Focal Length Comparison - Pure Ray Tracing\\nTilted Eye ‚Üí Spherical Checkerboard at 200mm', fontsize=16)
            plt.tight_layout()
            
            frame_path = f'/tmp/focal_frame_{i:03d}.png'
            plt.savefig(frame_path, dpi=120, bbox_inches='tight')
            plt.close()
            focal_frames.append(frame_path)
        
        focal_images = [Image.open(f) for f in focal_frames]
        focal_sweep_gif = '/tmp/ACTUAL_focal_length_sweep.gif'
        focal_images[0].save(focal_sweep_gif, save_all=True, append_images=focal_images[1:],
                            duration=800, loop=0, optimize=True)
        
        for f in focal_frames:
            os.remove(f)
        
        print(f"‚úÖ ACTUAL focal sweep GIF: {len(focal_images)} frames")
        
        # ACTUAL eye movement using proper ray tracing
        print("üö∂ Creating ACTUAL eye movement...")
        eye_frames = []
        x_positions = torch.linspace(-20, 20, 20, device=device)
        
        for i, x_pos in enumerate(x_positions):
            eye_pos = torch.tensor([x_pos.item(), 0.0, 0.0], device=device)
            
            with torch.no_grad():
                eye_view = render_eye_view(eye_pos, eye_focal_length, scene, params, resolution)
            
            plt.figure(figsize=(8, 8))
            plt.imshow(eye_view.cpu().numpy())
            plt.title(f'ACTUAL Eye View - X: {x_pos:.1f}mm\\nRetina Points at Sphere Center', fontsize=14)
            plt.axis('off')
            
            frame_path = f'/tmp/eye_frame_{i:03d}.png'
            plt.savefig(frame_path, dpi=120, bbox_inches='tight')
            plt.close()
            eye_frames.append(frame_path)
        
        eye_images = [Image.open(f) for f in eye_frames]
        eye_movement_gif = '/tmp/ACTUAL_eye_movement_sweep.gif'
        eye_images[0].save(eye_movement_gif, save_all=True, append_images=eye_images[1:],
                          duration=200, loop=0, optimize=True)
        
        for f in eye_frames:
            os.remove(f)
        
        print(f"‚úÖ ACTUAL eye movement GIF: {len(eye_images)} frames")
        
        # Upload ALL results
        progress_url = upload_to_catbox(progress_gif)
        displays_url = upload_to_catbox(displays_path)
        eye_views_url = upload_to_catbox(eye_views_path)
        focal_sweep_url = upload_to_catbox(focal_sweep_gif)
        eye_movement_url = upload_to_catbox(eye_movement_gif)
        
        print(f"\n" + "="*80)
        print("üì• ACTUAL SPHERICAL CHECKERBOARD RESULTS:")
        print(f"üé¨ Progress GIF ({iterations} frames): {progress_url}")
        print(f"üìä What Displays Show: {displays_url}")
        print(f"üëÅÔ∏è  What Eye Sees: {eye_views_url}")
        print(f"üéØ ACTUAL Focal Sweep: {focal_sweep_url}")
        print(f"üö∂ ACTUAL Eye Movement: {eye_movement_url}")
        print("="*80)
        
        return {
            'status': 'success',
            'message': f'ACTUAL spherical checkerboard optimization: {iterations} iterations, loss: {loss_history[-1]:.6f}',
            'final_loss': loss_history[-1],
            'progress_gif_url': progress_url,
            'displays_url': displays_url,
            'eye_views_url': eye_views_url,
            'focal_sweep_url': focal_sweep_url,
            'eye_movement_url': eye_movement_url,
            'all_download_urls': {
                'progress_gif': progress_url,
                'what_displays_show': displays_url,
                'what_eye_sees': eye_views_url,
                'actual_focal_sweep': focal_sweep_url,
                'actual_eye_movement': eye_movement_url
            },
            'optimization_specs': {
                'iterations': iterations,
                'resolution': resolution,
                'every_iteration_tracked': True,
                'actual_ray_tracing': True,
                'spherical_checkerboard': True
            },
            'timestamp': datetime.now().isoformat()
        }
        
    except Exception as e:
        import traceback
        return {
            'status': 'error',
            'message': str(e),
            'traceback': traceback.format_exc()
        }

runpod.serverless.start({"handler": handler})